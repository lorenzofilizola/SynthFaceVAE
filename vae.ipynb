{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FH1oXqqM7AU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ii81scn6M7AV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXGzlRFeM7AW"
   },
   "source": [
    "## Create a sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I3P_ef1wM7AX"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXkwy2aKM7AY"
   },
   "source": [
    "## Build the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 3)  84          ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 128, 8)  224         ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 16)   1168        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 32)   4640        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 64)   18496       ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 8, 8, 64)     36928       ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 4, 4, 128)    73856       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          1049088     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 512)          262656      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 512)          262656      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 512)          0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,709,796\n",
      "Trainable params: 1,709,796\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_encoder(filters, latent_dim=512, input_shape=(512,512,3)):\n",
    "\n",
    "\n",
    "    input_image = layers.Input(shape=input_shape, name='encoder_input')\n",
    "\n",
    "    x = layers.Conv2D(filters[0], 3, (2, 2), padding='same', activation='leaky_relu')(input_image)\n",
    "\n",
    "    for n_filters in filters[1:]:\n",
    "        x = layers.Conv2D(n_filters, 3, (2, 2), padding='same', activation='leaky_relu')(x)\n",
    "\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var', kernel_initializer='zeros')(x)\n",
    "\n",
    "\n",
    "    # use the reparameterization trick and get the output from the sample() function\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "    return keras.Model(input_image, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "encoder_filters = [3, 8, 16, 32, 64, 64, 128]\n",
    "encoder = build_encoder(encoder_filters, latent_dim=512)\n",
    "encoder.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 512)]             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 128)         36992     \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 8, 8, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 64)          73792     \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 32, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 32)        18464     \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 64, 64, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 64, 64, 16)        4624      \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 128, 128, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 128, 128, 8)       1160      \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 256, 256, 8)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 256, 256, 3)       219       \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSampling  (None, 512, 512, 3)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,179\n",
      "Trainable params: 172,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(filters, latent_dim=512, input_shape=(512,512,3)):\n",
    "    latent_input = layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "    #512/2^n where n is number of conv layers\n",
    "    x_dim = int(input_shape[0] / (2 ** len(filters)))\n",
    "    #dense_filters = latent_dim * 8\n",
    "    #x = layers.Dense(dense_filters, activation='relu')(latent_input)\n",
    "    x = layers.Reshape((x_dim, x_dim, int(latent_dim / x_dim**2)))(latent_input)\n",
    "\n",
    "    for n_filters in filters:\n",
    "        x = layers.Conv2D(n_filters, 3, (1, 1), padding='same', activation='tanh')(x)\n",
    "        x = layers.UpSampling2D()(x)\n",
    "\n",
    "    ##x = layers.Conv2DTranspose(3, (1, 1), (1, 1), padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    return keras.Model(latent_input, x, name='decoder')\n",
    "\n",
    "\n",
    "decoder_filters = [128, 64, 64, 32, 16, 8, 3]\n",
    "decoder = build_decoder(decoder_filters, latent_dim=512)\n",
    "decoder.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnHBo3e4M7Aa"
   },
   "source": [
    "## Define the VAE as a `Model` with a custom `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "caPQZeDqM7Ab"
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        z_mean, z_log_var, z = encoder(inputs)\n",
    "        return decoder(z)\n",
    "\n",
    "    def __init__(self, encoder, decoder, beta=1.0, loss='mse', **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.rec_loss = loss\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
    "        self.val_reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"val_reconstruction_loss\"\n",
    "        )\n",
    "        self.val_kl_loss_tracker = keras.metrics.Mean(name=\"val_kl_loss\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = 0\n",
    "            if self.rec_loss == 'mse':\n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        keras.losses.mse(data, reconstruction), axis=(1,2)\n",
    "                    )\n",
    "                )\n",
    "            elif self.rec_loss == 'bce':\n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\"wrong loss {loss}\".format(loss=self.rec_loss))\n",
    "\n",
    "            kl_loss = -self.beta * 0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            val_reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            val_kl_loss = -self.beta * 0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            val_kl_loss = tf.reduce_mean(tf.reduce_sum(val_kl_loss, axis=1))\n",
    "            val_total_loss = val_reconstruction_loss + val_kl_loss\n",
    "\n",
    "        self.val_total_loss_tracker.update_state(val_total_loss)\n",
    "        self.val_reconstruction_loss_tracker.update_state(val_reconstruction_loss)\n",
    "        self.val_kl_loss_tracker.update_state(val_kl_loss)\n",
    "        return {\n",
    "            \"total_loss\": self.val_total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.val_reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.val_kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0gHo7vkM7Ac"
   },
   "source": [
    "## Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Datasets\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Datasets\\\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('flickrfaces/dummy', output=\"flickrfaces/splits\", seed=1337, ratio=(.8, 0.1,0.1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "directory = \"flickrfaces\\splits\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - 0.5) * 2\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess,\n",
    "    #rescale=1./255,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    horizontal_flip=False, #True\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56000 images belonging to 1 classes.\n",
      "Found 7000 images belonging to 1 classes.\n",
      "Found 7048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_DIM = 512\n",
    "Y_DIM = 512\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        os.path.join(directory, 'train'),\n",
    "        target_size=(X_DIM, Y_DIM),\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        )\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        os.path.join(directory, 'val'),\n",
    "        target_size=(X_DIM, Y_DIM),\n",
    "        class_mode=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        seed=None,\n",
    "        )\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        os.path.join(directory, 'test'),\n",
    "        target_size=(X_DIM, Y_DIM),\n",
    "        class_mode=None,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        seed=None,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class VisualizeIOCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.original_batch = validation_generator.next()\n",
    "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
    "        original_batch = self.original_batch / 2 + 0.5\n",
    "        with self.file_writer.as_default():\n",
    "            images = np.reshape(original_batch[0:8], (-1, 512, 512, 3))\n",
    "            tf.summary.image(\"8 training input examples\", images, max_outputs=8, step=0)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        z_mean, z_log_var, z = encoder.predict(self.original_batch)\n",
    "        reconstructed = decoder.predict(z)\n",
    "        reconstructed = reconstructed / 2 + 0.5\n",
    "\n",
    "        # Using the file writer, log the reshaped image.\n",
    "        with self.file_writer.as_default():\n",
    "            images = np.reshape(reconstructed[0:8], (-1, 512, 512, 3))\n",
    "            tf.summary.image(\"8 training output examples\", images, max_outputs=8, step=epoch)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Checkpoint\n",
    "checkpoint_path = \"D:/Notebooks/Advanced_DL/Checkpoint/\"\n",
    "\n",
    "#if not os.path.exists(checkpoint_path):\n",
    "#    os.makedirs(checkpoint_path)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "callbacks.append(keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                                   min_delta=50,\n",
    "                                                   patience=5))\n",
    "\n",
    "now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_path, now_str + '.hdf5'),\n",
    "                              monitor='loss',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True,\n",
    "                              mode='auto',\n",
    "                              verbose=1)\n",
    "callbacks.append(cp_callback)\n",
    "\n",
    "log_dir = \"D:/Notebooks/Advanced_DL/logs/\" + now_str\n",
    "\n",
    "visualization_callback = VisualizeIOCallback(log_dir +\"/images\")\n",
    "callbacks.append(visualization_callback)\n",
    "\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0))\n",
    "\n",
    "# Early Stopping\n",
    "EARLY_STOP = False\n",
    "if EARLY_STOP:\n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor='val_total_loss',\n",
    "                                                   mode='auto',\n",
    "                                                   patience=10,\n",
    "                                                   verbose=1)\n",
    "    callbacks.append(es_callback)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-wUn41iM7Ac",
    "outputId": "6f9c2a84-001b-4e45-dcdb-20112fa6bf17",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 11195.5671 - reconstruction_loss: 9793.0215 - kl_loss: 1027.1321\n",
      "Epoch 42: loss improved from inf to 10820.15430, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 1s 54ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "438/438 [==============================] - 247s 548ms/step - loss: 11194.7119 - reconstruction_loss: 9793.0215 - kl_loss: 1027.1321 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10697.2816 - reconstruction_loss: 9661.4443 - kl_loss: 1028.1958\n",
      "Epoch 43: loss improved from 10820.15430 to 10689.64453, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 57ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "438/438 [==============================] - 240s 540ms/step - loss: 10697.2642 - reconstruction_loss: 9661.4443 - kl_loss: 1028.1958 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10675.1165 - reconstruction_loss: 9625.4199 - kl_loss: 1029.2085\n",
      "Epoch 44: loss improved from 10689.64453 to 10654.62402, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 52ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "438/438 [==============================] - 246s 550ms/step - loss: 10675.0698 - reconstruction_loss: 9625.4199 - kl_loss: 1029.2085 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10604.9439 - reconstruction_loss: 9576.5264 - kl_loss: 1030.8633\n",
      "Epoch 45: loss improved from 10654.62402 to 10607.38672, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 47ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "438/438 [==============================] - 243s 545ms/step - loss: 10604.9495 - reconstruction_loss: 9576.5264 - kl_loss: 1030.8633 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10599.3264 - reconstruction_loss: 9547.6436 - kl_loss: 1031.2675\n",
      "Epoch 46: loss improved from 10607.38672 to 10578.91016, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 52ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "438/438 [==============================] - 241s 538ms/step - loss: 10599.2799 - reconstruction_loss: 9547.6436 - kl_loss: 1031.2675 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10556.7377 - reconstruction_loss: 9499.6523 - kl_loss: 1032.0981\n",
      "Epoch 47: loss improved from 10578.91016 to 10531.75195, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 54ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "438/438 [==============================] - 234s 527ms/step - loss: 10556.6808 - reconstruction_loss: 9499.6523 - kl_loss: 1032.0981 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10533.9554 - reconstruction_loss: 9465.2676 - kl_loss: 1032.7097\n",
      "Epoch 48: loss improved from 10531.75195 to 10497.97266, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 50ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "438/438 [==============================] - 233s 522ms/step - loss: 10533.8734 - reconstruction_loss: 9465.2676 - kl_loss: 1032.7097 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10447.9705 - reconstruction_loss: 9412.0391 - kl_loss: 1033.6444\n",
      "Epoch 49: loss improved from 10497.97266 to 10445.69629, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 48ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "438/438 [==============================] - 240s 540ms/step - loss: 10447.9653 - reconstruction_loss: 9412.0391 - kl_loss: 1033.6444 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 10439.5804 - reconstruction_loss: 9398.1904 - kl_loss: 1034.2891\n",
      "Epoch 50: loss improved from 10445.69629 to 10432.48047, saving model to D:/Notebooks/Advanced_DL/Checkpoint\\20221011-200545.hdf5\n",
      "4/4 [==============================] - 0s 52ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "438/438 [==============================] - 253s 569ms/step - loss: 10439.5642 - reconstruction_loss: 9398.1904 - kl_loss: 1034.2891 - lr: 0.0010\n",
      "Epoch 51/200\n",
      " 77/438 [====>.........................] - ETA: 3:06 - loss: 10357.3046 - reconstruction_loss: 9360.0557 - kl_loss: 1036.2817"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder, beta=1)\n",
    "vae(train_generator.next())\n",
    "vae.load_weights(\"D:/Notebooks/Advanced_DL/Checkpoint/20221011-171930.hdf5\")\n",
    "#opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "opt = keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer=opt, loss=None)\n",
    "\n",
    "results = vae.fit(train_generator,\n",
    "                  epochs=200,\n",
    "                  callbacks = callbacks,\n",
    "                  #validation_data=validation_generator,\n",
    "                  initial_epoch=41,\n",
    "                  workers=8,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vae",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
